#!/usr/bin/env python3
"""
åŸºäºæ–‡æœ¬åˆ†æçš„æ–‡æœ¬åˆ†å‰²åˆ‡å—å·¥å…·
è¯¥å·¥å…·å°†Markdownæ–‡æ¡£æŒ‰ç…§æ–‡æœ¬å•å…ƒè¿›è¡Œåˆ†å‰²ï¼Œç¡®ä¿æ¯ä¸ªåˆ†å‰²å—çš„æ–‡æœ¬å®Œæ•´æ€§
"""

import os
import re
import json
from typing import List, Tuple
import argparse


class SemanticChunker:
    """æ–‡æœ¬æ–‡æœ¬åˆ†å‰²å™¨"""
    
    def __init__(self, chunk_min_length: int = 20, chunk_max_length: int = 200, 
                 overlap_min_length: int = 10, overlap_max_length: int = 50,
                 semantic_patterns: List[str] = None, enable_overlap: bool = True,
                 chunking_method: str = "semantic", chunk_by_length_config: dict = None,
                 chunk_by_paragraph_config: dict = None):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        
        Args:
            chunk_min_length: åˆ†å‰²å—æœ€å°å­—ç¬¦é•¿åº¦
            chunk_max_length: åˆ†å‰²å—æœ€å¤§å­—ç¬¦é•¿åº¦
            overlap_min_length: é‡å æ–‡æœ¬æœ€å°é•¿åº¦
            overlap_max_length: é‡å æ–‡æœ¬æœ€å¤§é•¿åº¦
            semantic_patterns: æ–‡æœ¬åˆ†å‰²ç‚¹çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åˆ—è¡¨
            enable_overlap: æ˜¯å¦å¯ç”¨é‡å æ–‡æœ¬åŠŸèƒ½
            chunking_method: åˆ†å—æ–¹å¼ ("semantic", "length", "paragraph")
            chunk_by_length_config: æŒ‰é•¿åº¦åˆ†å—çš„é…ç½®
            chunk_by_paragraph_config: æŒ‰æ®µè½åˆ†å—çš„é…ç½®
        """
        self.chunk_min_length = chunk_min_length
        self.chunk_max_length = chunk_max_length
        self.overlap_min_length = overlap_min_length
        self.overlap_max_length = overlap_max_length
        self.enable_overlap = enable_overlap
        self.chunking_method = chunking_method
        self.chunk_by_length_config = chunk_by_length_config or {}
        self.chunk_by_paragraph_config = chunk_by_paragraph_config or {}
        
        # å®šä¹‰æ–‡æœ¬åˆ†å‰²ç‚¹çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
        self.semantic_patterns = semantic_patterns if semantic_patterns else [
            r'\n#{1,6} ',           # æ ‡é¢˜
            r'\n###+',              # åˆ†éš”çº¿
            r'\n\n\*\*[^\n]+\*\*\n', # åŠ ç²—çš„æ®µè½æ ‡é¢˜
            r'\n\n[A-Z][^\n]*\n\n', # å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜ï¼ˆå¤§å†™å­—æ¯å¼€å¤´çš„ç‹¬ç«‹æ®µè½ï¼‰
        ]
        # æ ‡é¢˜è¯†åˆ«æ¨¡å¼
        self.title_patterns = [
            r'^#{1,6}\s.*$',           # Markdownæ ‡é¢˜
            r'^\*\*[^\n]+\*\*$',       # åŠ ç²—æ ‡é¢˜
        ]
    
    def read_markdown(self, file_path: str) -> str:
        """
        è¯»å–Markdownæ–‡ä»¶å†…å®¹
        
        Args:
            file_path: Markdownæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def detect_semantic_breaks(self, content: str) -> List[int]:
        """
        æ£€æµ‹æ–‡æœ¬åˆ†å‰²ç‚¹ä½ç½®
        
        Args:
            content: æ–‡æœ¬å†…å®¹
            
        Returns:
            åˆ†å‰²ç‚¹ä½ç½®åˆ—è¡¨
        """
        breaks = [0]  # å¼€å§‹ä½ç½®æ€»æ˜¯åˆ†å‰²ç‚¹
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬åˆ†å‰²ç‚¹
        for pattern in self.semantic_patterns:
            for match in re.finditer(pattern, content):
                pos = match.start() + 1  # +1 æ˜¯ä¸ºäº†è·³è¿‡å‰é¢çš„\n
                if pos not in breaks:
                    breaks.append(pos)
        
        # æ·»åŠ æ®µè½è¾¹ç•Œä½œä¸ºåˆ†å‰²ç‚¹ï¼ˆä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰
        paragraph_pattern = r'\n\s*\n'
        for match in re.finditer(paragraph_pattern, content):
            pos = match.end()
            if pos not in breaks:
                breaks.append(pos)
        
        # æ·»åŠ ç»“æŸä½ç½®
        breaks.append(len(content))
        
        # æŒ‰ä½ç½®æ’åº
        breaks.sort()
        return breaks
    
    def chunk_text(self, content: str) -> List[str]:
        """
        æ ¹æ®é…ç½®çš„åˆ†å—æ–¹å¼å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—
        
        Args:
            content: å¾…åˆ†å‰²çš„æ–‡æœ¬å†…å®¹
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if self.chunking_method == "length":
            return self.chunk_by_length(content)
        elif self.chunking_method == "paragraph":
            return self.chunk_by_paragraph(content)
        else:  # é»˜è®¤ä½¿ç”¨è¯­ä¹‰åˆ†å—
            return self.chunk_by_semantic(content)
    
    def chunk_by_length(self, content: str) -> List[str]:
        """
        æŒ‰å›ºå®šé•¿åº¦è¿›è¡Œæ–‡æœ¬åˆ†å‰²ï¼ŒåŒæ—¶æ­£ç¡®å¤„ç†æ ‡é¢˜
        
        Args:
            content: å¾…åˆ†å‰²çš„æ–‡æœ¬å†…å®¹
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        chunk_size = self.chunk_by_length_config.get("chunk_size", 100)
        chunk_overlap = self.chunk_by_length_config.get("chunk_overlap", 20)
        
        # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²å†…å®¹
        paragraphs = re.split(r'\n\s*\n', content)
        
        chunks = []
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
                
            cleaned_paragraph = paragraph.strip()
            
            # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦ä¸ºæ ‡é¢˜
            is_title = self._is_title_paragraph(cleaned_paragraph)
            
            if is_title:
                # æ ‡é¢˜ä½œä¸ºç‹¬ç«‹çš„ç©ºå—å¤„ç†
                chunks.append("")
            else:
                # éæ ‡é¢˜æ®µè½æŒ‰é•¿åº¦åˆ†å—
                start = 0
                para_length = len(cleaned_paragraph)
                
                while start < para_length:
                    end = min(start + chunk_size, para_length)
                    chunk = cleaned_paragraph[start:end]
                    chunks.append(chunk)
                    
                    # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªå—ï¼Œç§»åŠ¨èµ·å§‹ä½ç½®ä»¥åˆ›å»ºé‡å 
                    if end < para_length:
                        start = end - min(chunk_overlap, chunk_size // 2)
                    else:
                        break
        
        return chunks
    
    def chunk_by_paragraph(self, content: str) -> List[str]:
        """
        æŒ‰æ®µè½è¿›è¡Œæ–‡æœ¬åˆ†å‰²ï¼ŒåŒæ—¶æ­£ç¡®å¤„ç†æ ‡é¢˜
        
        Args:
            content: å¾…åˆ†å‰²çš„æ–‡æœ¬å†…å®¹
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        max_chunk_size = self.chunk_by_paragraph_config.get("max_chunk_size", 500)
        
        # æŒ‰æ®µè½åˆ†å‰²å†…å®¹ï¼ˆä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰
        paragraphs = re.split(r'\n\s*\n', content)
        
        chunks = []
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
            
            cleaned_paragraph = paragraph.strip()
            
            # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦ä¸ºæ ‡é¢˜
            is_title = self._is_title_paragraph(cleaned_paragraph)
            
            # å¦‚æœæ˜¯æ ‡é¢˜ï¼Œåˆ™å•ç‹¬ä½œä¸ºä¸€ä¸ªå—å¤„ç†ï¼ˆä½†å†…å®¹ä¸ºç©ºï¼‰
            if is_title:
                # æ ‡é¢˜å•ç‹¬ä½œä¸ºä¸€ä¸ªå—ï¼Œä½†å†…å®¹ä¸ºç©ºï¼ˆåªåœ¨ç´¢å¼•ä¸­ä¿ç•™æ ‡é¢˜ï¼‰
                chunks.append("")
                continue
            
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§å—å¤§å°ï¼Œåˆ™è¿›è¡Œå¥å­åˆ†å‰²
            if len(cleaned_paragraph) > max_chunk_size:
                # æŒ‰å¥å­åˆ†å‰²æ®µè½
                sentences = self._split_into_sentences(cleaned_paragraph)
                current_chunk = ""
                
                for sentence in sentences:
                    # å¦‚æœåŠ ä¸Šå½“å‰å¥å­ä¼šè¶…è¿‡æœ€å¤§å—å¤§å°ï¼Œä¸”å½“å‰å—å·²æœ‰å†…å®¹ï¼Œåˆ™ä¿å­˜å½“å‰å—
                    if current_chunk and len(current_chunk) + len(sentence) > max_chunk_size:
                        chunks.append(current_chunk.strip())
                        current_chunk = sentence
                    else:
                        # åˆå¹¶åˆ°å½“å‰å—
                        if current_chunk:
                            current_chunk += sentence
                        else:
                            current_chunk = sentence
                
                # ä¿å­˜æœ€åä¸€ä¸ªå—
                if current_chunk:
                    chunks.append(current_chunk.strip())
            else:
                # æ¯ä¸ªæ®µè½ä½œä¸ºä¸€ä¸ªç‹¬ç«‹çš„å—
                chunks.append(cleaned_paragraph)
        
        return chunks
    
    def chunk_by_semantic(self, content: str) -> List[str]:
        """
        åŸºäºæ–‡æœ¬åˆ†æè¿›è¡Œæ–‡æœ¬åˆ†å‰²ï¼Œä¼˜å…ˆä¿æŒæ®µè½å®Œæ•´æ€§
        
        Args:
            content: å¾…åˆ†å‰²çš„æ–‡æœ¬å†…å®¹
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        # æŒ‰æ®µè½åˆ†å‰²å†…å®¹ï¼ˆä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰
        paragraphs = re.split(r'\n\s*\n', content)
        
        chunks = []
        current_chunk = ""
        
        # æŒ‰æ®µè½è¿›è¡Œåˆ†ç»„ï¼Œä¼˜å…ˆä¿æŒæ®µè½å®Œæ•´æ€§
        i = 0
        while i < len(paragraphs):
            paragraph = paragraphs[i]
            if not paragraph.strip():
                i += 1
                continue
                
            cleaned_paragraph = paragraph.strip()
            
            # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦åŒ…å«base64å›¾ç‰‡ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
            # ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…å›¾ç‰‡ï¼Œæ”¯æŒå¤šè¡ŒåŒ¹é…
            base64_images = re.findall(r'!\[.*?\]\(data:image/.*?;base64.*?\)', cleaned_paragraph, re.DOTALL)
            if base64_images:
                # å¦‚æœå½“å‰å—ä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜å½“å‰å—
                if current_chunk and current_chunk.strip():
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # ç‰¹æ®Šå¤„ç†ï¼šå°†åŒ…å«å›¾ç‰‡çš„æ®µè½æŒ‰å›¾ç‰‡åˆ†å‰²æˆç‹¬ç«‹çš„å—
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²ï¼Œä¿ç•™åˆ†éš”ç¬¦
                parts = re.split(r'(!\[.*?\]\(data:image/.*?;base64.*?\))', cleaned_paragraph, flags=re.DOTALL)
                
                # å¤„ç†åˆ†å‰²åçš„éƒ¨åˆ†
                for part in parts:
                    if part.strip():
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
                        if re.match(r'!\[.*?\]\(data:image/.*?;base64.*?\)', part.strip(), re.DOTALL):
                            # è¿™æ˜¯ä¸€ä¸ªbase64å›¾ç‰‡ï¼Œå•ç‹¬ä½œä¸ºä¸€ä¸ªå—
                            chunks.append(part.strip())
                        else:
                            # è¿™æ˜¯æ™®é€šæ–‡æœ¬éƒ¨åˆ†ï¼Œéœ€è¦è¿›ä¸€æ­¥å¤„ç†
                            # å¦‚æœæ–‡æœ¬éƒ¨åˆ†ä¸ä¸ºç©ºï¼ŒæŒ‰æ­£å¸¸é€»è¾‘å¤„ç†
                            if part.strip():
                                text_chunks = self._process_text_content(part.strip())
                                chunks.extend(text_chunks)
                
                i += 1
                continue
            
            # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦ä¸ºæ ‡é¢˜
            is_title = self._is_title_paragraph(cleaned_paragraph)
            
            # å¦‚æœæ˜¯æ ‡é¢˜ï¼Œåˆ™å•ç‹¬ä½œä¸ºä¸€ä¸ªå—å¤„ç†ï¼ˆä½†å†…å®¹ä¸ºç©ºï¼‰
            if is_title:
                # å¦‚æœå½“å‰å—ä¸ä¸ºç©ºï¼Œå…ˆä¿å­˜å½“å‰å—
                if current_chunk and current_chunk.strip():
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                # æ ‡é¢˜å•ç‹¬ä½œä¸ºä¸€ä¸ªå—ï¼Œä½†å†…å®¹ä¸ºç©ºï¼ˆåªåœ¨ç´¢å¼•ä¸­ä¿ç•™æ ‡é¢˜ï¼‰
                chunks.append("")
                i += 1
                continue
            
            # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦ä»¥åˆ—è¡¨é¡¹å¼€å§‹
            is_list_item = bool(re.match(r'^([\*\-\+]\s|\d+\.\s)', cleaned_paragraph))
            
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½åè¶…è¿‡æœ€å¤§é•¿åº¦
            if len(current_chunk) + len(cleaned_paragraph) > self.chunk_max_length:
                # å¦‚æœå½“å‰å—å·²è¾¾åˆ°æœ€å°é•¿åº¦ï¼Œåˆ™ä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
                if len(current_chunk) >= self.chunk_min_length:
                    chunks.append(current_chunk.strip())
                    current_chunk = cleaned_paragraph
                else:
                    # å¦‚æœå½“å‰å—æœªè¾¾åˆ°æœ€å°é•¿åº¦ï¼Œå°è¯•åˆå¹¶
                    if current_chunk:
                        current_chunk += "\n\n" + cleaned_paragraph
                    else:
                        current_chunk = cleaned_paragraph
                    
                    # å¦‚æœåˆå¹¶åè¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™å¼ºåˆ¶åˆ†å‰²
                    if len(current_chunk) > self.chunk_max_length:
                        # å¦‚æœæ˜¯åˆ—è¡¨é¡¹ï¼Œå°½é‡ä¿æŒå®Œæ•´æ€§
                        if is_list_item:
                            chunks.append(current_chunk.strip())
                            current_chunk = ""
                        else:
                            # å¦åˆ™æŒ‰å¥å­åˆ†å‰²
                            sentences = self._split_into_sentences(current_chunk)
                            temp_chunk = ""
                            for sentence in sentences:
                                if len(temp_chunk) + len(sentence) > self.chunk_max_length or len(temp_chunk) < self.chunk_min_length:
                                    if temp_chunk:
                                        chunks.append(temp_chunk.strip())
                                    temp_chunk = sentence
                            if temp_chunk:
                                current_chunk = temp_chunk
                            else:
                                current_chunk = ""
            else:
                # åˆå¹¶åˆ°å½“å‰å—
                if current_chunk:
                    current_chunk += "\n\n" + cleaned_paragraph
                else:
                    current_chunk = cleaned_paragraph
            i += 1
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk and current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        # åå¤„ç†ï¼šå¤„ç†è¿‡é•¿çš„å—
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= self.chunk_max_length:
                final_chunks.append(chunk)
            else:
                # å¯¹äºä»ç„¶è¿‡é•¿çš„å—ï¼ŒæŒ‰æ®µè½è¿›è¡Œåˆ†å‰²
                final_chunks.extend(self._split_long_chunk_by_paragraph(chunk))
        
        # å¤„ç†é‡å æ–‡æœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_overlap and len(final_chunks) > 1:
            final_chunks = self._add_overlap_to_chunks(final_chunks)
        
        return final_chunks
    
    def _process_text_content(self, text_content: str) -> List[str]:
        """
        å¤„ç†çº¯æ–‡æœ¬å†…å®¹ï¼ŒæŒ‰æ­£å¸¸åˆ†å—é€»è¾‘è¿›è¡Œåˆ†å‰²
        
        Args:
            text_content: çº¯æ–‡æœ¬å†…å®¹
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text_content.strip():
            return []
            
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºç­‰äºæœ€å¤§å—é•¿åº¦ï¼Œç›´æ¥è¿”å›
        if len(text_content) <= self.chunk_max_length:
            return [text_content]
            
        # å¦‚æœæ–‡æœ¬é•¿åº¦è¶…è¿‡æœ€å¤§å—é•¿åº¦ï¼ŒæŒ‰å¥å­åˆ†å‰²
        sentences = self._split_into_sentences(text_content)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            if len(current_chunk) + len(sentence) > self.chunk_max_length and len(current_chunk) >= self.chunk_min_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += sentence
                else:
                    current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks if chunks else [text_content]
    
    def _add_overlap_to_chunks(self, chunks: List[str]) -> List[str]:
        """
        ä¸ºæ–‡æœ¬å—æ·»åŠ é‡å æ–‡æœ¬ä»¥ä¿è¯å®Œæ•´æ€§ï¼ŒåŒæ—¶ä¿æŒå†…å®¹è¿ç»­æ€§
        
        Args:
            chunks: åŸå§‹æ–‡æœ¬å—åˆ—è¡¨
            
        Returns:
            æ·»åŠ é‡å æ–‡æœ¬åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            if i == 0:
                # ç¬¬ä¸€ä¸ªå—ï¼Œåªå‘åæ·»åŠ é‡å 
                next_chunk = chunks[i + 1]
                overlap_text = self._extract_overlap_text(chunk, next_chunk, is_forward=True)
                overlapped_chunks.append(chunk + ("\n\n" + overlap_text if overlap_text else ""))
            elif i == len(chunks) - 1:
                # æœ€åä¸€ä¸ªå—ï¼Œåªå‘å‰æ·»åŠ é‡å 
                prev_chunk = chunks[i - 1]
                overlap_text = self._extract_overlap_text(prev_chunk, chunk, is_forward=False)
                overlapped_chunks.append((overlap_text + "\n\n" if overlap_text else "") + chunk)
            else:
                # ä¸­é—´çš„å—ï¼Œå‰åéƒ½æ·»åŠ é‡å 
                prev_chunk = chunks[i - 1]
                next_chunk = chunks[i + 1]
                
                # å‘å‰æ·»åŠ é‡å ï¼ˆç¡®ä¿ä¸é‡å¤å·²æœ‰å†…å®¹ï¼‰
                forward_overlap = self._extract_overlap_text(prev_chunk, chunk, is_forward=False)
                # å‘åæ·»åŠ é‡å ï¼ˆç¡®ä¿ä¸é‡å¤å·²æœ‰å†…å®¹ï¼‰
                backward_overlap = self._extract_overlap_text(chunk, next_chunk, is_forward=True)
                
                # é¿å…é‡å¤å†…å®¹ï¼Œåªæ·»åŠ å¿…è¦çš„é‡å éƒ¨åˆ†
                overlapped_chunk = chunk
                if forward_overlap and not chunk.startswith(forward_overlap.strip()):
                    overlapped_chunk = forward_overlap + "\n\n" + overlapped_chunk
                if backward_overlap and not chunk.endswith(backward_overlap.strip()):
                    overlapped_chunk = overlapped_chunk + "\n\n" + backward_overlap
                
                overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks
    
    def _extract_overlap_text(self, prev_chunk: str, next_chunk: str, is_forward: bool = True) -> str:
        """
        ä»ç›¸é‚»æ–‡æœ¬å—ä¸­æå–é‡å æ–‡æœ¬
        
        Args:
            prev_chunk: å‰ä¸€ä¸ªæ–‡æœ¬å—
            next_chunk: åä¸€ä¸ªæ–‡æœ¬å—
            is_forward: æ˜¯å¦å‘å‰æ·»åŠ é‡å ï¼ˆFalseè¡¨ç¤ºå‘åæ·»åŠ é‡å ï¼‰
            
        Returns:
            æå–çš„é‡å æ–‡æœ¬
        """
        # ç¡®å®šé‡å é•¿åº¦
        overlap_length = min(self.overlap_max_length, len(prev_chunk) if is_forward else len(next_chunk))
        overlap_length = max(overlap_length, self.overlap_min_length)
        
        if is_forward:
            # ä»åä¸€ä¸ªå—çš„å¼€å¤´æå–é‡å æ–‡æœ¬
            return next_chunk[:overlap_length] if len(next_chunk) >= overlap_length else next_chunk
        else:
            # ä»å‰ä¸€ä¸ªå—çš„ç»“å°¾æå–é‡å æ–‡æœ¬
            return prev_chunk[-overlap_length:] if len(prev_chunk) >= overlap_length else prev_chunk
    
    def _split_long_chunk_by_paragraph(self, chunk: str) -> List[str]:
        """
        æŒ‰æ®µè½åˆ†å‰²è¿‡é•¿çš„æ–‡æœ¬å—
        
        Args:
            chunk: è¿‡é•¿çš„æ–‡æœ¬å—
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆä¸¤ä¸ªæ¢è¡Œç¬¦ï¼‰
        paragraphs = re.split(r'\n\s*\n', chunk)
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
                
            # æ¸…ç†æ®µè½
            cleaned_paragraph = paragraph.strip()
            
            if len(current_chunk) + len(cleaned_paragraph) > self.chunk_max_length and len(current_chunk) >= self.chunk_min_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = cleaned_paragraph
            else:
                if current_chunk:
                    current_chunk += "\n\n" + cleaned_paragraph
                else:
                    current_chunk = cleaned_paragraph
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        # å¦‚æœä»ç„¶æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„å—ï¼Œä½¿ç”¨å¥å­åˆ†å‰²
        final_chunks = []
        for ch in chunks:
            if len(ch) <= self.chunk_max_length:
                final_chunks.append(ch)
            else:
                final_chunks.extend(self._split_long_chunk(ch))
        
        return final_chunks if final_chunks else [chunk]
    
    def _split_long_chunk(self, chunk: str) -> List[str]:
        """
        åˆ†å‰²è¿‡é•¿çš„æ–‡æœ¬å—
        
        Args:
            chunk: è¿‡é•¿çš„æ–‡æœ¬å—
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', chunk)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if not sentence.strip():
                continue
                
            if len(current_chunk) + len(sentence) > self.chunk_max_length and len(current_chunk) >= self.chunk_min_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                if current_chunk:
                    current_chunk += sentence
                else:
                    current_chunk = sentence
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks if chunks else [chunk]
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        å°†æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²
        
        Args:
            text: å¾…åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„å¥å­åˆ—è¡¨
        """
        # ä¸­è‹±æ–‡å¥å­åˆ†å‰²çš„æ­£åˆ™è¡¨è¾¾å¼
        sentence_endings = r'[.!?ã€‚ï¼ï¼Ÿ]+'
        sentences = re.split(f'({sentence_endings})', text)
        
        # åˆå¹¶å¥å­å’Œæ ‡ç‚¹ç¬¦å·
        combined_sentences = []
        i = 0
        while i < len(sentences):
            if i + 1 < len(sentences) and re.match(sentence_endings, sentences[i + 1]):
                combined_sentences.append(sentences[i] + sentences[i + 1])
                i += 2
            else:
                if sentences[i].strip():  # åªæ·»åŠ éç©ºå¥å­
                    combined_sentences.append(sentences[i])
                i += 1
        
        return [s.strip() for s in combined_sentences if s.strip()]
    
    def _is_title_paragraph(self, paragraph: str) -> bool:
        """
        åˆ¤æ–­æ®µè½æ˜¯å¦ä¸ºæ ‡é¢˜
        
        Args:
            paragraph: æ®µè½å†…å®¹
            
        Returns:
            æ˜¯å¦ä¸ºæ ‡é¢˜
        """
        for pattern in self.title_patterns:
            if re.match(pattern, paragraph.strip()):
                return True
        return False
    
    def _extract_meaningful_title(self, chunk: str) -> str:
        """
        ä»æ–‡æœ¬å—ä¸­æå–æœ‰æ„ä¹‰çš„æ ‡é¢˜
        
        Args:
            chunk: æ–‡æœ¬å—
            
        Returns:
            æå–çš„æ ‡é¢˜
        """
        # å¦‚æœæ–‡æœ¬å—ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ ‡é¢˜
        if not chunk or not chunk.strip():
            return "æ–‡æœ¬æ®µè½"
            
        lines = chunk.strip().split('\n')
        
        # å¦‚æœç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜æ ¼å¼ï¼Œä½¿ç”¨å®ƒä½œä¸ºæ ‡é¢˜
        if lines and self._is_title_paragraph(lines[0]):
            title = re.sub(r'[#*\[\]]', '', lines[0]).strip()
            # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            title = re.sub(r'[<>:"/\\|?*\x00-\x1f]|data:image/[a-z]+;base64[^\s]*|\![\(][^\)]*[\)]', '', title).strip()
            return title[:30]  # é™åˆ¶é•¿åº¦
        
        # å¦åˆ™ä½¿ç”¨å‰30ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
        first_line = lines[0] if lines else "æ–‡æœ¬æ®µè½"
        # ç§»é™¤å¯èƒ½å¯¼è‡´æ–‡ä»¶åé—®é¢˜çš„å­—ç¬¦
        safe_title = re.sub(r'[<>:"/\\|?*\x00-\x1f]|data:image/[a-z]+;base64[^\s]*|\![\(][^\)]*[\)]', '', first_line).strip()
        return safe_title[:30] or "æ–‡æœ¬æ®µè½"
    
    def process_file(self, input_file: str, output_dir: str = None, save_chunk_index: bool = False, chunk_index_filename: str = "chunk_index.json") -> List[str]:
        """
        å¤„ç†å•ä¸ªMarkdownæ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥çš„Markdownæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä¿å­˜åˆ°æ–‡ä»¶
            save_chunk_index: æ˜¯å¦ä¿å­˜åˆ†å—ç´¢å¼•
            chunk_index_filename: åˆ†å—ç´¢å¼•æ–‡ä»¶å
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬å—åˆ—è¡¨
        """
        print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {os.path.basename(input_file)}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            content = self.read_markdown(input_file)
            print(f"  å·²è¯»å–æ–‡ä»¶å†…å®¹ï¼Œå…± {len(content)} ä¸ªå­—ç¬¦")
        except Exception as e:
            print(f"  è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
            return []
        
        # è¿›è¡Œæ–‡æœ¬åˆ†å‰²
        print(f"  æ­£åœ¨è¿›è¡Œ{self.chunking_method}æ–‡æœ¬åˆ†å‰²...")
        chunks = self.chunk_text(content)
        print(f"  åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºç›®å½•ï¼‰
        if output_dir:
            base_filename = os.path.splitext(os.path.basename(input_file))[0]
            specific_output_dir = os.path.join(output_dir, base_filename)
            print(f"  æ­£åœ¨ä¿å­˜æ–‡æœ¬å—åˆ°ç›®å½•: {specific_output_dir}")
            self.save_chunks(chunks, specific_output_dir, base_filename, save_chunk_index, chunk_index_filename, content)
            print(f"  âœ“ æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±ä¿å­˜ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        return chunks
    
    def save_chunks(self, chunks: List[str], output_dir: str, base_filename: str, save_chunk_index: bool = False, chunk_index_filename: str = "chunk_index.json", original_content: str = ""):
        """
        ä¿å­˜åˆ†å‰²åçš„æ–‡æœ¬å—åˆ°æ–‡ä»¶
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            base_filename: åŸºç¡€æ–‡ä»¶å
            save_chunk_index: æ˜¯å¦ä¿å­˜åˆ†å—ç´¢å¼•
            chunk_index_filename: åˆ†å—ç´¢å¼•æ–‡ä»¶å
            original_content: åŸå§‹å†…å®¹ï¼Œç”¨äºæå–æ ‡é¢˜
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # æ¸…ç©ºå·²å­˜åœ¨çš„åˆ†å‰²ç»“æœ
        if os.path.exists(output_dir):
            existing_files = [f for f in os.listdir(output_dir) if os.path.isfile(os.path.join(output_dir, f))]
            if existing_files:
                print(f"  æ¸…ç†è¾“å‡ºç›®å½•ï¼Œåˆ é™¤ {len(existing_files)} ä¸ªå·²å­˜åœ¨çš„æ–‡ä»¶")
                for file in existing_files:
                    file_path = os.path.join(output_dir, file)
                    os.remove(file_path)
        
        # ç”¨äºå­˜å‚¨åˆ†å—ç´¢å¼•ä¿¡æ¯
        chunk_index = []
        
        # å¦‚æœæœ‰åŸå§‹å†…å®¹ï¼Œé¢„å…ˆæå–æ‰€æœ‰æ ‡é¢˜
        titles = []
        is_title_list = []
        if original_content:
            titles, is_title_list = self._extract_titles_and_flags_from_content(original_content)
        
        # ä¿å­˜æ¯ä¸ªæ–‡æœ¬å—åˆ°ç´¢å¼•æ–‡ä»¶ä¸­ï¼Œè€Œä¸æ˜¯å•ç‹¬çš„æ–‡ä»¶
        print(f"  æ­£åœ¨ä¿å­˜ {len(chunks)} ä¸ªæ–‡æœ¬å—åˆ°ç´¢å¼•æ–‡ä»¶...")
        
        # å›¾ç‰‡IDè®¡æ•°å™¨ï¼Œä»1å¼€å§‹
        image_id_counter = 1
        
        for i, chunk in enumerate(chunks, 1):
            # ç”Ÿæˆæœ‰æ„ä¹‰çš„æ ‡é¢˜
            if i <= len(titles):
                title = titles[i-1]
            else:
                title = self._extract_meaningful_title(chunk)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆä»…ç”¨äºæ ‡è¯†ï¼‰
            filename = f"{i:03d}_{title}.md"
            
            # è®°å½•ç´¢å¼•ä¿¡æ¯ï¼Œç›´æ¥åŒ…å«æ–‡æœ¬å†…å®¹
            if save_chunk_index:
                # ç¡®å®šæ˜¯å¦ä¸ºæ ‡é¢˜å—
                is_title_block = i <= len(is_title_list) and is_title_list[i-1]
                
                # å¯¹äºæ ‡é¢˜å—ï¼Œæˆ‘ä»¬å­˜å‚¨æ ‡é¢˜ä¿¡æ¯
                index_title = title if is_title_block else ""
                
                # å¯¹äºç©ºå†…å®¹çš„å—ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
                content = chunk if chunk.strip() else ""
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡å—ï¼ˆä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ï¼‰
                is_image_block = False
                image_content = ""
                
                # æ£€æŸ¥æ•´ä¸ªå—æ˜¯å¦å°±æ˜¯ä¸€å¼ å›¾ç‰‡
                if re.match(r'^!\[.*?\]\(data:image/.*?;base64.*?\)$', chunk.strip(), re.DOTALL):
                    is_image_block = True
                    image_content = chunk.strip()
                
                # å‡†å¤‡ç´¢å¼•æ¡ç›®
                index_entry = {
                    "id": len(chunk_index) + 1,
                    "title": index_title,
                    "content": image_content if is_image_block else content,  # å›¾ç‰‡å—å­˜å‚¨å›¾ç‰‡å†…å®¹ï¼Œå…¶ä»–å—å­˜å‚¨æ–‡æœ¬å†…å®¹
                    "is_title": is_title_block  # æ·»åŠ æ ‡é¢˜æ ‡è®°å­—æ®µ
                }
                
                # å¦‚æœæ˜¯å›¾ç‰‡å—ï¼Œæ·»åŠ image_idå±æ€§å’Œtypeå±æ€§
                if is_image_block:
                    index_entry["image_id"] = image_id_counter
                    index_entry["type"] = "image"
                    image_id_counter += 1
                else:
                    index_entry["type"] = "text"
                
                chunk_index.append(index_entry)
        
        # ä¿å­˜åˆ†å—ç´¢å¼•æ–‡ä»¶
        if save_chunk_index and chunk_index:
            index_filepath = os.path.join(output_dir, chunk_index_filename)
            try:
                with open(index_filepath, 'w', encoding='utf-8') as f:
                    json.dump(chunk_index, f, ensure_ascii=False, indent=2)
                print(f"  âœ“ å·²ä¿å­˜åˆ†å—ç´¢å¼•æ–‡ä»¶: {chunk_index_filename} (åŒ…å« {len(chunk_index)} ä¸ªæ¡ç›®)")
            except Exception as e:
                print(f"  ä¿å­˜ç´¢å¼•æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        elif save_chunk_index:
            print("  âš ï¸ æœªç”Ÿæˆç´¢å¼•ä¿¡æ¯ï¼Œè·³è¿‡ç´¢å¼•æ–‡ä»¶ä¿å­˜")
    
    def _extract_titles_and_flags_from_content(self, content: str) -> Tuple[List[str], List[bool]]:
        """
        ä»åŸå§‹å†…å®¹ä¸­æå–æ‰€æœ‰æ ‡é¢˜å’Œæ ‡é¢˜æ ‡è®°
        
        Args:
            content: åŸå§‹å†…å®¹
            
        Returns:
            æ ‡é¢˜åˆ—è¡¨å’Œæ˜¯å¦ä¸ºæ ‡é¢˜çš„æ ‡è®°åˆ—è¡¨
        """
        titles = []
        is_title_list = []
        # æŒ‰æ®µè½åˆ†å‰²å†…å®¹
        paragraphs = re.split(r'\n\s*\n', content)
        
        for paragraph in paragraphs:
            if not paragraph.strip():
                continue
                
            cleaned_paragraph = paragraph.strip()
            
            # æ£€æŸ¥å½“å‰æ®µè½æ˜¯å¦ä¸ºæ ‡é¢˜
            is_title = self._is_title_paragraph(cleaned_paragraph)
            is_title_list.append(is_title)
            
            if is_title:
                # æå–æ ‡é¢˜æ–‡æœ¬ï¼ˆå»é™¤Markdownæ ‡è®°ï¼‰
                title = re.sub(r'[#*\[\]]', '', cleaned_paragraph).strip()
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                title = re.sub(r'[<>:"/\\|?*\x00-\x1f]|data:image/[a-z]+;base64[^\s]*|\![\(][^\)]*[\)]', '', title).strip()
                titles.append(title[:30])  # é™åˆ¶é•¿åº¦
            else:
                # å¯¹äºéæ ‡é¢˜æ®µè½ï¼Œä½¿ç”¨å‰30ä¸ªå­—ç¬¦ä½œä¸ºæ–‡ä»¶å
                first_line = cleaned_paragraph.split('\n')[0] if '\n' in cleaned_paragraph else cleaned_paragraph
                # ç§»é™¤å¯èƒ½å¯¼è‡´æ–‡ä»¶åé—®é¢˜çš„å­—ç¬¦
                safe_title = re.sub(r'[<>:"/\\|?*\x00-\x1f]|data:image/[a-z]+;base64[^\s]*|\![\(][^\)]*[\)]', '', first_line).strip()
                titles.append(safe_title[:30] or "æ–‡æœ¬æ®µè½")
        
        return titles, is_title_list
    
    def process_directory(self, input_dir: str, output_dir: str, save_chunk_index: bool = False, chunk_index_filename: str = "chunk_index.json"):
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰Markdownæ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            save_chunk_index: æ˜¯å¦ä¿å­˜åˆ†å—ç´¢å¼•
            chunk_index_filename: åˆ†å—ç´¢å¼•æ–‡ä»¶å
        """
        if not os.path.exists(input_dir):
            print(f"âŒ é”™è¯¯: è¾“å…¥ç›®å½• '{input_dir}' ä¸å­˜åœ¨")
            return
        
        # é€’å½’è·å–æ‰€æœ‰Markdownæ–‡ä»¶ï¼ˆé€‚åº”æ–°çš„ç›®å½•ç»“æ„ï¼‰
        md_files = []
        for root, dirs, files in os.walk(input_dir):
            for file in files:
                if file.lower().endswith('.md'):
                    # æ„å»ºç›¸å¯¹äºè¾“å…¥ç›®å½•çš„è·¯å¾„
                    full_path = os.path.join(root, file)
                    relative_path = os.path.relpath(full_path, input_dir)
                    md_files.append((relative_path, full_path))
        
        if not md_files:
            print(f"âš ï¸  æç¤º: åœ¨ç›®å½• '{input_dir}' ä¸­æœªæ‰¾åˆ°Markdownæ–‡ä»¶")
            return
        
        print(f"ğŸ“ æ‰¾åˆ° {len(md_files)} ä¸ªMarkdownæ–‡ä»¶ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
        print("-" * 50)
        
        success_count = 0
        failed_count = 0
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, (relative_path, full_path) in enumerate(md_files, 1):
            print(f"[{i}/{len(md_files)}] å¤„ç†æ–‡ä»¶: {relative_path}")
            try:
                self.process_file(full_path, output_dir, save_chunk_index, chunk_index_filename)
                success_count += 1
            except Exception as e:
                print(f"  âŒ å¤„ç†æ–‡ä»¶ '{relative_path}' æ—¶å‡ºé”™: {str(e)}")
                failed_count += 1
        
        print("-" * 50)
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"  âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ªæ–‡ä»¶")
        if failed_count > 0:
            print(f"  âŒ å¤„ç†å¤±è´¥: {failed_count} ä¸ªæ–‡ä»¶")
        print(f"  ğŸ“¦ è¾“å‡ºç›®å½•: {output_dir}")

def find_config_file(config_file: str, script_dir: str) -> str:
    """
    åŠ¨æ€æŸ¥æ‰¾é…ç½®æ–‡ä»¶è·¯å¾„
    
    Args:
        config_file: é…ç½®æ–‡ä»¶å
        script_dir: è„šæœ¬æ‰€åœ¨ç›®å½•
        
    Returns:
        é…ç½®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    # æ£€æŸ¥å‡ ç§å¯èƒ½çš„è·¯å¾„
    possible_paths = [
        os.path.join(script_dir, config_file),  # å½“å‰ç›®å½•
        os.path.join(script_dir, '..', config_file),  # ä¸Šçº§ç›®å½•
        os.path.join(script_dir, '..', '..', config_file),  # ä¸Šä¸Šçº§ç›®å½•
        config_file  # ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„è·¯å¾„
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return config_file  # è¿”å›åŸå§‹è·¯å¾„ï¼Œè®©ç³»ç»Ÿå†³å®š

def load_config(config_file: str = "chunk_config.json") -> dict:
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½åˆ†å—å‚æ•°
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«åˆ†å—å‚æ•°çš„å­—å…¸
    """
        # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # åŠ¨æ€æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config_path = find_config_file(config_file, script_dir)
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except Exception as e:
            print(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    
    # é»˜è®¤å‚æ•°
    return {
        "chunk_min_length": 50,
        "chunk_max_length": 200,
        "overlap_min_length": 10,
        "overlap_max_length": 50,
        "semantic_patterns": [
            "\\n#{1,6} ",           
            "\\n###+",              
            "\\n\\n\\*\\*[^\\n]+\\*\\*\\n", 
            "\\n\\n[A-Z][^\\n]*\\n\\n" 
        ],
        "enable_overlap": True,
        "save_chunk_index": True,
        "chunk_index_filename": "chunk_index.json"
    }


def main():
    print("ğŸ”¤ åŸºäºè¯­ä¹‰åˆ†æçš„æ–‡æœ¬åˆ†å‰²å·¥å…·")
    print("=" * 40)
    
    # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    config = load_config()
    print(f"âš™ï¸  å·²åŠ è½½é…ç½®å‚æ•°")
    
    # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤è¾“å…¥è¾“å‡ºè·¯å¾„
    default_input_path = config.get("default_input_path", "md_ConvertResult")
    default_output_path = config.get("default_output_path", "md_SemanticSliced")
    
    # ä»é…ç½®æ–‡ä»¶è·å–åˆ†å—æ–¹å¼é…ç½®
    chunking_method = config.get("chunking_method", "semantic")
    chunk_by_length_config = config.get("chunk_by_length", {})
    chunk_by_paragraph_config = config.get("chunk_by_paragraph", {})
    
    parser = argparse.ArgumentParser(description='åŸºäºæ–‡æœ¬åˆ†æçš„æ–‡æœ¬åˆ†å‰²å·¥å…·')
    parser.add_argument('-i', '--input', default=default_input_path, help=f'è¾“å…¥ç›®å½•è·¯å¾„æˆ–å•ä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: {default_input_path}ï¼‰')
    parser.add_argument('-o', '--output', default=default_output_path, help=f'è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: {default_output_path}ï¼‰')
    parser.add_argument('--min-length', type=int, default=config["chunk_min_length"], help='åˆ†å‰²å—æœ€å°å­—ç¬¦é•¿åº¦ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--max-length', type=int, default=config["chunk_max_length"], help='åˆ†å‰²å—æœ€å¤§å­—ç¬¦é•¿åº¦ï¼ˆé»˜è®¤: 200ï¼‰')
    parser.add_argument('--overlap-min', type=int, default=config.get("overlap_min_length", 10), help='é‡å æ–‡æœ¬æœ€å°é•¿åº¦ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--overlap-max', type=int, default=config.get("overlap_max_length", 50), help='é‡å æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--enable-overlap', action='store_true', default=config.get("enable_overlap", True), help='å¯ç”¨é‡å æ–‡æœ¬åŠŸèƒ½ï¼ˆé»˜è®¤: å¯ç”¨ï¼‰')
    parser.add_argument('--save-index', action='store_true', default=config.get("save_chunk_index", True), help='ä¿å­˜åˆ†å—ç´¢å¼•æ–‡ä»¶ï¼ˆé»˜è®¤: ä¿å­˜ï¼‰')
    parser.add_argument('--index-filename', default=config.get("chunk_index_filename", "chunk_index.json"), help='åˆ†å—ç´¢å¼•æ–‡ä»¶åï¼ˆé»˜è®¤: chunk_index.jsonï¼‰')
    parser.add_argument('--chunking-method', default=chunking_method, choices=['semantic', 'length', 'paragraph'], help=f'åˆ†å—æ–¹å¼ï¼ˆé»˜è®¤: {chunking_method}ï¼‰')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œåˆ™ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    if args.config:
        print(f"ğŸ“„ ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶: {args.config}")
        config = load_config(args.config)
        # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ²¡æœ‰æŒ‡å®šï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
        if args.min_length == 50:  # é»˜è®¤å€¼
            args.min_length = config.get("chunk_min_length", 50)
        if args.max_length == 200:  # é»˜è®¤å€¼
            args.max_length = config.get("chunk_max_length", 200)
        if args.overlap_min == 10:  # é»˜è®¤å€¼
            args.overlap_min = config.get("overlap_min_length", 10)
        if args.overlap_max == 50:  # é»˜è®¤å€¼
            args.overlap_max = config.get("overlap_max_length", 50)
        if not args.enable_overlap:  # å¦‚æœæœªåœ¨å‘½ä»¤è¡ŒæŒ‡å®šï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
            args.enable_overlap = config.get("enable_overlap", True)
        if not args.save_index:  # å¦‚æœæœªåœ¨å‘½ä»¤è¡ŒæŒ‡å®šï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶
            args.save_index = config.get("save_chunk_index", True)
        if args.index_filename == "chunk_index.json":  # é»˜è®¤å€¼
            args.index_filename = config.get("chunk_index_filename", "chunk_index.json")
        if args.chunking_method == chunking_method:  # å¦‚æœä½¿ç”¨é»˜è®¤å€¼
            args.chunking_method = config.get("chunking_method", "semantic")
    
    # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„å‚æ•°
    print(f"ğŸ“Š å½“å‰å‚æ•°è®¾ç½®:")
    print(f"   æœ€å°å—é•¿åº¦: {args.min_length}")
    print(f"   æœ€å¤§å—é•¿åº¦: {args.max_length}")
    print(f"   é‡å æ–‡æœ¬: {'å¯ç”¨' if args.enable_overlap else 'ç¦ç”¨'}")
    if args.enable_overlap:
        print(f"   é‡å èŒƒå›´: {args.overlap_min}-{args.overlap_max} å­—ç¬¦")
    print(f"   ç´¢å¼•æ–‡ä»¶: {'ä¿å­˜' if args.save_index else 'ä¸ä¿å­˜'}")
    if args.save_index:
        print(f"   ç´¢å¼•æ–‡ä»¶å: {args.index_filename}")
    print(f"   åˆ†å—æ–¹å¼: {args.chunking_method}")
    
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    chunker = SemanticChunker(
        chunk_min_length=args.min_length,
        chunk_max_length=args.max_length,
        overlap_min_length=args.overlap_min,
        overlap_max_length=args.overlap_max,
        semantic_patterns=config.get("semantic_patterns"),
        enable_overlap=args.enable_overlap,
        chunking_method=args.chunking_method,
        chunk_by_length_config=chunk_by_length_config,
        chunk_by_paragraph_config=chunk_by_paragraph_config
    )
    
    # æ„å»ºå®Œæ•´çš„è¾“å…¥è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨ç›¸å¯¹äºè„šæœ¬çš„è·¯å¾„ï¼‰
    input_path = os.path.join(script_dir, args.input)
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºå•ä¸ªæ–‡ä»¶
    if os.path.isfile(input_path) and input_path.lower().endswith('.md'):
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        print("ğŸ“„ æ£€æµ‹åˆ°å•ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        chunks = chunker.process_file(input_path, args.output, args.save_index, args.index_filename)
        if chunks:
            print(f"âœ… å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        else:
            print("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥")
    elif os.path.isfile(args.input) and args.input.lower().endswith('.md'):
        # å¦‚æœæä¾›äº†ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹è·¯å¾„çš„æ–‡ä»¶
        print("ğŸ“„ æ£€æµ‹åˆ°å•ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")
        chunks = chunker.process_file(args.input, args.output, args.save_index, args.index_filename)
        if chunks:
            print(f"âœ… å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        else:
            print("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥")
    else:
        # å¤„ç†ç›®å½•
        print("ğŸ“ æ£€æµ‹åˆ°ç›®å½•ï¼Œå¼€å§‹æ‰¹é‡å¤„ç†...")
        # å¯¹äºè¾“å‡ºç›®å½•ä¹Ÿä½¿ç”¨ç›¸å¯¹äºè„šæœ¬çš„è·¯å¾„
        output_path = args.output
        if not os.path.isabs(output_path):
            output_path = os.path.join(script_dir, output_path)
        chunker.process_directory(args.input, output_path, args.save_index, args.index_filename)


    print("=" * 40)
    print("ğŸ‰ æ–‡æœ¬åˆ†å‰²å·¥å…·æ‰§è¡Œå®Œæ¯•")

def process_all_documents(input_dir: str, output_dir: str, config_file: str = None):
    """
    å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£
    
    Args:
        input_dir (str): è¾“å…¥ç›®å½•è·¯å¾„
        output_dir (str): è¾“å‡ºç›®å½•è·¯å¾„
        config_file (str): é…ç½®æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½é…ç½®
    config = load_chunk_config(config_file) if config_file else {}
    
    # åº”ç”¨é…ç½®
    chunk_min_length = config.get("chunk_min_length", 10)
    chunk_max_length = config.get("chunk_max_length", 150)
    overlap_min_length = config.get("overlap_min_length", 10)
    overlap_max_length = config.get("overlap_max_length", 50)
    semantic_patterns = config.get("semantic_patterns", None)
    enable_overlap = config.get("enable_overlap", False)
    save_chunk_index = config.get("save_chunk_index", True)
    chunk_index_filename = config.get("chunk_index_filename", "chunk_index.json")
    chunking_method = config.get("chunking_method", "semantic")
    chunk_by_length_config = config.get("chunk_by_length", {})
    chunk_by_paragraph_config = config.get("chunk_by_paragraph", {})
    
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    chunker = SemanticChunker(
        chunk_min_length=chunk_min_length,
        chunk_max_length=chunk_max_length,
        overlap_min_length=overlap_min_length,
        overlap_max_length=overlap_max_length,
        semantic_patterns=semantic_patterns,
        enable_overlap=enable_overlap,
        chunking_method=chunking_method,
        chunk_by_length_config=chunk_by_length_config,
        chunk_by_paragraph_config=chunk_by_paragraph_config
    )
    
    # å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£
    chunker.process_directory(input_dir, output_dir, save_chunk_index, chunk_index_filename)


if __name__ == "__main__":
    main()