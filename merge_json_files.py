#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Merge JSON files from converted_data and sliced_data directories.

This script merges the pic_index.json files from converted_data with the 
chunk_index.json files from sliced_data to create a unified JSON structure
that contains both text chunks and images in document order.
"""

import os
import json
import base64
import argparse
import re
from pathlib import Path


def find_config_file(config_file: str, script_dir: str) -> str:
    """
    åŠ¨æ€æŸ¥æ‰¾é…ç½®æ–‡ä»¶è·¯å¾„
    
    Args:
        config_file: é…ç½®æ–‡ä»¶å
        script_dir: è„šæœ¬æ‰€åœ¨ç›®å½•
        
    Returns:
        é…ç½®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    """
    # æ£€æŸ¥å‡ ç§å¯èƒ½çš„è·¯å¾„
    possible_paths = [
        os.path.join(script_dir, config_file),  # å½“å‰ç›®å½•
        os.path.join(script_dir, '..', config_file),  # ä¸Šçº§ç›®å½•
        os.path.join(script_dir, '..', '..', config_file),  # ä¸Šä¸Šçº§ç›®å½•
        config_file  # ç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„è·¯å¾„
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return path
    
    return config_file  # è¿”å›åŸå§‹è·¯å¾„ï¼Œè®©ç³»ç»Ÿå†³å®š


def load_merge_config(config_file: str = "merge_config.json") -> dict:
    """
    ä»é…ç½®æ–‡ä»¶åŠ è½½åˆå¹¶å‚æ•°
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«åˆå¹¶å‚æ•°çš„å­—å…¸
    """
    # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # åŠ¨æ€æŸ¥æ‰¾é…ç½®æ–‡ä»¶
    config_path = find_config_file(config_file, script_dir)
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        except Exception as e:
            print(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    
    # é»˜è®¤å‚æ•°
    return {
        "default_input_converted_path": "converted_data",
        "default_input_sliced_path": "sliced_data",
        "default_output_path": "merged_data",
        "enable_base64_processing": True,
        "save_merged_index": False,
        "merged_index_filename": "merged_index.json"
    }


def load_json_file(file_path):
    """Load JSON file and return its content."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return None


def save_json_file(data, file_path):
    """Save data to JSON file."""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"Error saving {file_path}: {e}")
        return False


def load_image_as_base64(image_path):
    """Load image file and return its base64 encoding."""
    try:
        with open(image_path, 'rb') as f:
            return base64.b64encode(f.read()).decode('utf-8')
    except Exception as e:
        print(f"Error loading image {image_path}: {e}")
        return None


def extract_base64_from_text(text):
    """Extract base64 data from markdown image syntax in text."""
    pattern = r'!\[.*?\]\(data:image/.*?;base64,([^)]+)\)'
    matches = re.findall(pattern, text)
    return matches


def process_text_with_base64_images(items):
    """
    Process items to handle text entries with base64 image placeholders.
    Convert text entries with base64 placeholders to image entries.
    """
    processed_items = []
    id_counter = 1
    
    for item in items:
        if item['type'] == 'text' and item.get('content'):
            # Check if text contains base64 image placeholders
            base64_matches = extract_base64_from_text(item['content'])
            
            if base64_matches:
                # Split text by base64 placeholders
                parts = re.split(r'!\[.*?\]\(data:image/.*?;base64,[^)]+\)', item['content'])
                
                # Process each part and insert images where placeholders were
                for i, part in enumerate(parts):
                    # Add text part if it's not empty
                    if part.strip():
                        text_item = {
                            'id': id_counter,
                            'type': 'text',
                            'title': item.get('title', ''),
                            'content': part.strip(),
                            'length': len(part.strip())
                        }
                        processed_items.append(text_item)
                        id_counter += 1
                    
                    # Add image item if there's a corresponding base64 match
                    if i < len(base64_matches):
                        image_item = {
                            'id': id_counter,
                            'type': 'image',
                            'base64': base64_matches[i],
                            'original_name': f"embedded_image_{id_counter}.png",
                            'path': f"embedded_images/embedded_image_{id_counter}.png",
                            'size': len(base64_matches[i])
                        }
                        processed_items.append(image_item)
                        id_counter += 1
            else:
                # No base64 placeholders, keep the item as is
                item['id'] = id_counter
                processed_items.append(item)
                id_counter += 1
        else:
            # Not a text item or no content, keep as is
            item['id'] = id_counter
            processed_items.append(item)
            id_counter += 1
    
    return processed_items


def find_document_files(converted_dir, sliced_dir):
    """
    Find all document-related files in the new directory structure.
    
    Args:
        converted_dir (str): Path to converted_data directory
        sliced_dir (str): Path to sliced_data directory
        
    Returns:
        dict: Dictionary mapping document names to their file paths
    """
    document_files = {}
    
    # éå†sliced_dataç›®å½•ä¸­çš„æ‰€æœ‰å­ç›®å½•ï¼Œå› ä¸ºè¿™æ˜¯å¤„ç†æµç¨‹çš„æœ€åä¸€æ­¥
    # å³ä½¿æ²¡æœ‰å›¾ç‰‡ï¼Œä¹Ÿåº”è¯¥å¤„ç†è¿™äº›æ–‡æ¡£
    for root, dirs, files in os.walk(sliced_dir):
        for dir_name in dirs:
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨chunk_index.jsonæ–‡ä»¶
            chunk_index_path = os.path.join(root, dir_name, 'chunk_index.json')
            if os.path.exists(chunk_index_path):
                doc_name = dir_name
                if doc_name not in document_files:
                    document_files[doc_name] = {}
                document_files[doc_name]['chunk_index'] = chunk_index_path
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨images_index.jsonæ–‡ä»¶
                images_index_path = os.path.join(converted_dir, doc_name, 'images_index.json')
                if os.path.exists(images_index_path):
                    document_files[doc_name]['images_index'] = images_index_path
    
    return document_files


def merge_document_data(converted_dir, sliced_dir, output_dir, enable_base64_processing=True, save_merged_index=False):
    """
    Merge converted_data and sliced_data JSON files for all documents.
    
    Args:
        converted_dir (str): Path to converted_data directory
        sliced_dir (str): Path to sliced_data directory
        output_dir (str): Path to output directory
        enable_base64_processing (bool): Whether to process base64 image placeholders
        save_merged_index (bool): Whether to save the merged index file
    """
    # Create output directory if it doesn't exist
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Find all document files in the new directory structure
    document_files = find_document_files(converted_dir, sliced_dir)
    
    print(f"Found {len(document_files)} documents to process")
    
    # ç”¨äºä¿å­˜æ‰€æœ‰åˆå¹¶æ–‡ä»¶çš„ç´¢å¼•
    merged_index = {}
    
    for doc_name, files in document_files.items():
        print(f"Processing: {doc_name}")
        
        # Load images_index.json (contains base64 data) if it exists
        image_index_data = {}
        if 'images_index' in files:
            image_index_data = load_json_file(files['images_index']) or {}
            
        # Load chunk_index.json
        chunk_index_path = files['chunk_index']
        chunk_index_data = load_json_file(chunk_index_path)
        if chunk_index_data is None:
            print(f"  Warning: chunk_index.json not found for {doc_name}")
            continue
            
        # Create a mapping from image_id to base64 data
        image_id_to_base64 = {}
        for image_filename, image_info in image_index_data.items():
            image_id = image_info.get('image_id')
            base64_data = image_info.get('base64')
            if image_id is not None and base64_data is not None:
                image_id_to_base64[image_id] = base64_data
                
        # Process chunk_index_data to add base64 data where image_id matches
        merged_data = []
        for chunk in chunk_index_data:
            # Copy the original chunk data
            new_chunk = chunk.copy()
            
            # If this chunk has an image_id, check if we have base64 data for it
            if 'image_id' in chunk:
                image_id = chunk['image_id']
                if image_id in image_id_to_base64:
                    # Add the base64 data to this chunk
                    new_chunk['base64'] = image_id_to_base64[image_id]
                    
            merged_data.append(new_chunk)
            
        # ä¿®æ”¹ä¿å­˜é€»è¾‘ï¼šä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶å¤¹ï¼Œå¹¶åœ¨å…¶ä¸­ä¿å­˜result.json
        # åˆ›å»ºæ–‡æ¡£å¯¹åº”çš„æ–‡ä»¶å¤¹
        doc_output_dir = os.path.join(output_dir, doc_name)
        Path(doc_output_dir).mkdir(parents=True, exist_ok=True)
        
        # åœ¨æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­ä¿å­˜result.json
        output_file = os.path.join(doc_output_dir, "result.json")
        if save_json_file(merged_data, output_file):
            print(f"  Saved: {output_file}")
            # æ·»åŠ åˆ°åˆå¹¶ç´¢å¼•
            if save_merged_index:
                merged_index[doc_name] = {
                    "file": f"{doc_name}/result.json",
                    "items_count": len(merged_data)
                }
        else:
            print(f"  Failed to save: {output_file}")
    
    # ä¿å­˜åˆå¹¶ç´¢å¼•æ–‡ä»¶
    if save_merged_index and merged_index:
        index_file = os.path.join(output_dir, "merged_index.json")
        if save_json_file(merged_index, index_file):
            print(f"  Saved merged index: {index_file}")


def main():
    """Main function."""
    print("ğŸ™Œ JSONæ–‡ä»¶åˆå¹¶å·¥å…·")
    print("=" * 40)
    
    # è·å–å½“å‰è„šæœ¬çš„ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°
    config = load_merge_config()
    print(f"âš™ï¸  å·²åŠ è½½é…ç½®å‚æ•°")
    
    # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤è¾“å…¥è¾“å‡ºè·¯å¾„
    default_converted_path = config.get("default_input_converted_path", "converted_data")
    default_sliced_path = config.get("default_input_sliced_path", "sliced_data")
    default_output_path = config.get("default_output_path", "merged_data")
    default_enable_base64_processing = config.get("enable_base64_processing", True)
    default_save_merged_index = config.get("save_merged_index", False)
    default_merged_index_filename = config.get("merged_index_filename", "merged_index.json")
    
    parser = argparse.ArgumentParser(description='Merge converted_data and sliced_data JSON files')
    parser.add_argument('--converted-dir', default=default_converted_path,
                        help=f'Path to converted_data directory (default: {default_converted_path})')
    parser.add_argument('--sliced-dir', default=default_sliced_path,
                        help=f'Path to sliced_data directory (default: {default_sliced_path})')
    parser.add_argument('--output-dir', default=default_output_path,
                        help=f'Path to output directory (default: {default_output_path})')
    parser.add_argument('--enable-base64-processing', action='store_true', 
                        default=default_enable_base64_processing,
                        help=f'Enable base64 image processing (default: {default_enable_base64_processing})')
    parser.add_argument('--disable-base64-processing', action='store_false', 
                        dest='enable_base64_processing',
                        help='Disable base64 image processing')
    parser.add_argument('--save-merged-index', action='store_true',
                        default=None,
                        help='Save merged index file')
    parser.add_argument('--no-save-merged-index', action='store_false',
                        dest='save_merged_index',
                        help='Do not save merged index file')
    parser.add_argument('--config', help='Configuration file path')
    
    args = parser.parse_args()
    
    # å¤„ç†save_merged_indexå‚æ•°
    if args.save_merged_index is not None:
        # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šäº†save_merged_indexï¼Œåˆ™ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
        save_merged_index = args.save_merged_index
    else:
        # å¦åˆ™ä»é…ç½®æ–‡ä»¶è·å–å‚æ•°
        if args.config:
            print(f"ğŸ“„ ä½¿ç”¨æŒ‡å®šé…ç½®æ–‡ä»¶: {args.config}")
            config = load_merge_config(args.config)
            # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ²¡æœ‰æŒ‡å®šï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
            if args.converted_dir == default_converted_path:  # é»˜è®¤å€¼
                args.converted_dir = config.get("default_input_converted_path", "converted_data")
            if args.sliced_dir == default_sliced_path:  # é»˜è®¤å€¼
                args.sliced_dir = config.get("default_input_sliced_path", "sliced_data")
            if args.output_dir == default_output_path:  # é»˜è®¤å€¼
                args.output_dir = config.get("default_output_path", "merged_data")
            if args.enable_base64_processing == default_enable_base64_processing:  # é»˜è®¤å€¼
                args.enable_base64_processing = config.get("enable_base64_processing", True)
            # ä»é…ç½®æ–‡ä»¶è·å–save_merged_indexå‚æ•°
            save_merged_index = config.get("save_merged_index", False)
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šé…ç½®æ–‡ä»¶ï¼Œåˆ™ç›´æ¥ä»ä¸»é…ç½®è·å–å‚æ•°
            save_merged_index = config.get("save_merged_index", False)
    
    # æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„å‚æ•°
    print(f"ğŸ“Š å½“å‰å‚æ•°è®¾ç½®:")
    print(f"   è½¬æ¢æ•°æ®ç›®å½•: {args.converted_dir}")
    print(f"   åˆ‡ç‰‡æ•°æ®ç›®å½•: {args.sliced_dir}")
    print(f"   è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"   Base64å¤„ç†: {'å¯ç”¨' if args.enable_base64_processing else 'ç¦ç”¨'}")
    print(f"   ä¿å­˜åˆå¹¶ç´¢å¼•: {'å¯ç”¨' if save_merged_index else 'ç¦ç”¨'}")
    
    # Check if directories exist
    if not os.path.exists(args.converted_dir):
        print(f"Error: Converted data directory '{args.converted_dir}' does not exist")
        return
        
    if not os.path.exists(args.sliced_dir):
        print(f"Error: Sliced data directory '{args.sliced_dir}' does not exist")
        return
    
    print("Starting merge process...")
    merge_document_data(args.converted_dir, args.sliced_dir, args.output_dir, args.enable_base64_processing, save_merged_index)
    print("Merge process completed.")


if __name__ == '__main__':
    main()